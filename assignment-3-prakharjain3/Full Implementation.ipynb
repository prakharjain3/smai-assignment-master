{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions in Neural Networks\n",
    "\n",
    "Activation functions are a crucial component of artificial neural networks, used to introduce non-linearity into the model. They determine whether a neuron should be activated (fired) or not based on the weighted sum of its inputs. Different activation functions serve various purposes and have unique characteristics. Here are some of the most common activation functions:\n",
    "\n",
    "### Step/Threshold Function:\n",
    "\n",
    "- **Formula:** $f(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$\n",
    "- This function was historically used but is rarely used in modern neural networks due to its discontinuity.\n",
    "\n",
    "### Sigmoid Function:\n",
    "\n",
    "- **Formula:** $f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- **Range:** $(0, 1)$\n",
    "- It squashes the input into a range between 0 and 1. It is used in the output layer of binary classification problems.\n",
    "\n",
    "### Hyperbolic Tangent (tanh) Function:\n",
    "\n",
    "- **Formula:** $f(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$\n",
    "- **Range:** $(-1, 1)$\n",
    "- Similar to the sigmoid function, but it squashes the input into a range between -1 and 1.\n",
    "\n",
    "### Rectified Linear Unit (ReLU):\n",
    "\n",
    "- **Formula:** $f(x) = \\max(0, x)$\n",
    "- **Range:** $[0, âˆž)$\n",
    "- It is one of the most widely used activation functions due to its simplicity and effectiveness. It helps mitigate the vanishing gradient problem.\n",
    "\n",
    "### Leaky ReLU:\n",
    "\n",
    "- **Formula:** $f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{else} \\end{cases}$ (where $\\alpha$ is a small positive constant, typically around 0.01)\n",
    "- It addresses the \"dying ReLU\" problem by allowing a small gradient when $x < 0$.\n",
    "\n",
    "### Parametric ReLU (PReLU):\n",
    "\n",
    "- **Formula:** $f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{else} \\end{cases}$ (where $\\alpha$ is a learnable parameter)\n",
    "- Similar to Leaky ReLU but with $\\alpha$ as a learnable parameter during training.\n",
    "\n",
    "### Exponential Linear Unit (ELU):\n",
    "\n",
    "- **Formula:** $f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha(e^{x} - 1) & \\text{else} \\end{cases}$ (where $\\alpha$ is a positive constant)\n",
    "- It addresses the vanishing gradient problem and has some advantages over ReLU, but it's computationally more expensive.\n",
    "\n",
    "### Swish:\n",
    "\n",
    "- **Formula:** $f(x) = x \\cdot \\text{sigmoid}(x)$\n",
    "- Swish is a smooth approximation of ReLU and has been found to perform well in some cases.\n",
    "\n",
    "### Softmax:\n",
    "\n",
    "- Used in the output layer of multi-class classification problems.\n",
    "- **Formula:** $f(x)_i = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$ for all $i$, where $i$ is the current class and $j$ ranges over all classes.\n",
    "\n",
    "These activation functions have different properties and can be chosen based on the specific requirements and characteristics of the neural network and the problem being solved. ReLU and its variants are among the most popular choices in deep learning due to their training efficiency and effectiveness in mitigating gradient-related issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "self.opt_dict = {\n",
    "    \"sgd\": SGD(),\n",
    "    \"bgd\": BGD(),\n",
    "    \"mbsgd\" : MBGD()\n",
    "    # TODO : will implement these someday\n",
    "    # \"adagrad\": Adagrad(),\n",
    "    # \"adadelta\": Adadelta(),\n",
    "    # \"rmsprop\": RMSProp(),\n",
    "    # \"adam\": Adam(),\n",
    "    # \"nadam\": Nadam()\n",
    "    # \"adamax\": Adamax()\n",
    "    # \"momentum\": Momentum()\n",
    "    # \"nag\": NesterovAcceleratedGradient()\n",
    "    # \"lbfgs\": LBFGS()\n",
    "    # \"rprop\": RProp()\n",
    "    # \"yf\": YurrisFriend()\n",
    "    # \"la\": LookAhead()\n",
    "    # \"ranger\" : Ranger()\n",
    "    # \"FTRL\": FollowTheRegularizedLeader()\n",
    "    \n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources for Back Propagation\n",
    "\n",
    "https://cs231n.github.io/optimization-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
