{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is for The Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a neural network is an iterative process. In every iteration, we do a pass forward through a model’s layers to compute an output for each training example in a batch of data. Then another pass proceeds backward through the layers, propagating how much each parameter affects the final output by computing a gradient with respect to each parameter. The average gradient for the batch, the parameters, and some per-parameter optimization state is passed to an optimization algorithm, such as Adam, which computes the next iteration’s parameters (which should have slightly better performance on your data) and new per-parameter optimization state. As the training iterates over batches of data, the model evolves to produce increasingly accurate outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Layer Perceptron Classification\n",
    "In this part, you are required to implement MLP classification from scratch\n",
    "using numpy, pandas and experiment with various activation functions and\n",
    "optimization techniques, evaluate the model’s performance, visualize decision\n",
    "boundaries, and draw comparisons with the previously implemented multino-\n",
    "mial logistic regression.\n",
    "2.1 Model Building from Scratch\n",
    "Build an MLP classifier class with the following specifications: Do not\n",
    "use sklearn for this.\n",
    "1. Create a class where you can modify and access the learning rate, activa-\n",
    "tion function, optimisers, number of hidden layers and neurons.\n",
    "2. Implement methods for forward propagation, backpropagation, and train-\n",
    "ing.\n",
    "3. Different activation functions introduce non-linearity to the model and\n",
    "affect the learning process. Implement the Sigmoid, Tanh, and ReLU\n",
    "activation functions and make them easily interchangeable within your\n",
    "MLP framework.\n",
    "3\n",
    "4. Optimization techniques dictate how the neural network updates its weights\n",
    "during training. Implement methods for the Stochastic Gradient Descent\n",
    "(SGD), Batch Gradient Descent, and Mini-Batch Gradient Descent algo-\n",
    "rithms from scratch, ensuring that they can be employed within your MLP\n",
    "architecture. Additionally, draw comparisons with the inbuilt ADAM op-\n",
    "timizer in terms of performance.\n",
    "\n",
    "\n",
    "what is the activation function for output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from icecream import ic\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes For Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid: # Sigmoid Function:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def activation(self, z):\n",
    "        return self.sigmoid(z)\n",
    "    def activation_derivative(self, s):\n",
    "        # s = self.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "\n",
    "\n",
    "class Tanh: # Hyperbolic Tangent (Tanh) Function    \n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    def activation(self, z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    def activation_derivative(self, t):\n",
    "        # t = np.tanh(z)\n",
    "        return 1 - t**2 \n",
    "\n",
    "        \n",
    "class ReLU: # Rectified Linear Unit (ReLU)\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    def activation(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def activation_derivative(self, z):\n",
    "        return (z > 0)\n",
    "\n",
    "class Identity:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def activation(self, z):\n",
    "        # Linear activation function (identity function)\n",
    "        return z\n",
    "\n",
    "    def activation_derivative(self, z):\n",
    "        # Derivative of the linear activation function is always 1\n",
    "        return np.ones_like(z)\n",
    "    \n",
    "\n",
    "# Used for Classification\n",
    "class Softmax:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    def activation(self, z):\n",
    "        # print(z.shape)\n",
    "        exps = np.exp(z - np.max(z))\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "    \n",
    "    def activation_derivative(self, z):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes For Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def __init__(self, epsilon = 1e-15) -> None:\n",
    "        self.epsilon = epsilon\n",
    "    def loss(self, y_pred, y_true):\n",
    "        epsilon = 1e-10\n",
    "        loss = -np.sum(y_true * np.log(y_pred+epsilon)) / y_true.shape[0]\n",
    "        return loss\n",
    "    def gradient(self, y_pred, y_true):\n",
    "\n",
    "        return ((y_pred - y_true) / y_true.shape[0])\n",
    "    \n",
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        # Calculate the squared differences between predicted and true values\n",
    "        squared_errors = (y_pred - y_true) ** 2\n",
    "        # Compute the mean of squared errors\n",
    "        mse = np.mean(squared_errors)\n",
    "        return mse\n",
    "\n",
    "    def gradient(self, y_pred, y_true):\n",
    "        # Compute the gradient of the MSE loss\n",
    "        gradient = 2 * (y_pred - y_true) / len(y_true)\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes For Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a^{(1)} = X$$\n",
    "$$\\hat{y} = a^{(L)} = \\sigma(z^{(L)})$$\n",
    "$$a^{(l) = \\sigma(z^{(l)})}$$\n",
    "$$z^{(l)} = W^{(l - 1)}$$\n",
    "$$\\delta^{(L)} = \\hat{y} - y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrence Relation for $\\delta^{(l)}$\n",
    "\n",
    "$$\\delta^{(l)} = \\delta^{(l + 1)}(W^{(l)})^T \\odot \\sigma'(z^{(l)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "   \\frac{\\partial J}{\\partial W^{(l)}} = \\frac{1}{m} \\delta^{(l + 1)}(a^{(l)})^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    # no_of_neurons is equal to the no_of_output of the layer\n",
    "    def __init__(self, no_of_inputs, no_of_neurons, activation) -> None:\n",
    "        # use xavier initialization\n",
    "        self.weights = np.random.randn(no_of_inputs, no_of_neurons) * np.sqrt(1 / no_of_inputs)\n",
    "        self.bias = np.zeros((1, no_of_neurons))\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        # self.inputs represents the activations of the previous layer a^{(l)}    \n",
    "\n",
    "        \n",
    "        self.a = self.activation.activation(np.dot(self.inputs, self.weights) + self.bias) \n",
    "        self.f_prime_z = self.activation.activation_derivative(self.a)\n",
    "        return self.a\n",
    "      \n",
    "    \n",
    "    def backward(self, delta, weights):\n",
    "        # Element-wise multiplication for the derivative of the activation function\n",
    "        \n",
    "        # 1/m is already included in the gradient of the loss function\n",
    "        delta = np.dot(delta, weights.T) * self.f_prime_z\n",
    "                \n",
    "        self.dj_dw = np.dot(self.inputs.T, delta)\n",
    "        self.dj_db = np.sum(delta, axis=0, keepdims=True)\n",
    "        \n",
    "        \n",
    "        return delta\n",
    "    \n",
    "    def update_weights(self, learning_rate):\n",
    "        # Update weights and bias\n",
    "        self.weights -= learning_rate * self.dj_dw\n",
    "        self.bias -= learning_rate * self.dj_db\n",
    "    \n",
    "class InputLayer:\n",
    "    def __init__(self, input_size) -> None: # input_size is unnecessary here\n",
    "        self.f_prime_z = None\n",
    "        self.weights = None\n",
    "    def forward(self, inputs):\n",
    "        # Forward pass for the input layer is just passing the input data\n",
    "        return inputs\n",
    "\n",
    "    def backward(self, delta):\n",
    "        # The input layer has no weights, so the gradient is not modified\n",
    "        return delta\n",
    "    def update_weights(self, learning_rate):\n",
    "        pass\n",
    "\n",
    "class OutputLayer:\n",
    "    def __init__(self, no_of_inputs, no_of_neurons, activation) -> None:\n",
    "        self.weights = np.random.randn(no_of_inputs, no_of_neurons) * np.sqrt(1 / no_of_inputs)\n",
    "        self.bias = np.zeros((1, no_of_neurons))\n",
    "        self.f_prime_z = None\n",
    "        self.activation = activation\n",
    "        # ic(activation)\n",
    "        \n",
    "        # ic(self.weights.shape)\n",
    "        \n",
    "        # if no_of_neurons == 1:\n",
    "        #     self.activation = Identity()\n",
    "        # else:\n",
    "        #     self.activation = Softmax()\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        # self.z = np.dot(self.inputs, self.weights)\n",
    "        # self.a = self.activation.activation(np.dot(self.inputs, self.weights))\n",
    "        self.a = self.activation.activation(np.dot(self.inputs, self.weights)+self.bias)\n",
    "        return self.a\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        # grad_output is the gradient of the loss function with respect to the output of the layer\n",
    "        # The gradient of the loss with respect to the input of the layer\n",
    "        \n",
    "        # self.dzdW = inputs.T\n",
    "        # basically self.inputs.T is the same as self.dzdW\n",
    "        self.dj_dw = np.dot(self.inputs.T, delta)\n",
    "        self.dj_db = np.sum(delta, axis=0, keepdims=True)\n",
    "        \n",
    "        # grad_weights = np.dot(self.inputs.T, grad_output)\n",
    "        # grad_bias = np.sum(grad_output, axis=0, keepdims=True)\n",
    "\n",
    "        # The gradients of the loss with respect to the parameters of the layer\n",
    "        \n",
    "        # delta last layer\n",
    "        return delta\n",
    "    \n",
    "    def update_weights(self, learning_rate):\n",
    "        # Update weights and bias\n",
    "        self.weights -= learning_rate * self.dj_dw\n",
    "        self.bias -= learning_rate * self.dj_db\n",
    "    \n",
    "    # update will look something like this\n",
    "    # def update(self, learning_rate):\n",
    "    #     # Update weights and bias\n",
    "    #     self.weights -= learning_rate * dj_dw\n",
    "    #     self.bias -= learning_rate * self.grad_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes For Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add classes for optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_layer_sizes, output_size,\\\n",
    "                 activation_function, output_activation_function, optimizer, loss, learning_rate = 0.001) -> None:\n",
    "        \"\"\"\n",
    "        input_size: number of features in the input\n",
    "        \n",
    "        hidden_layer_sizes: list of number of neurons in each hidden layer\n",
    "        \n",
    "        output_size: number of classes in the output\n",
    "        \n",
    "        activation_function: list of activation function for each layer\n",
    "        \n",
    "        learning_rate: learning rate for gradient \n",
    "        \"\"\"\n",
    "        \n",
    "        self.act_dict = {\n",
    "            \"sigmoid\": Sigmoid(),\n",
    "            \"tanh\": Tanh(),\n",
    "            \"relu\": ReLU(),\n",
    "            \"identity\": Identity(),\n",
    "            \"softmax\": Softmax()\n",
    "        }\n",
    "        \n",
    "        \n",
    "        # # initial plan was to implement classes for the optimizers\n",
    "        # self.opt_dict = {\n",
    "        #     \"sgd\": SGD(),\n",
    "        #     \"bgd\": BGD(),\n",
    "        #     \"mbsgd\" : MBGD()\n",
    "        # }\n",
    "        \n",
    "                \n",
    "        for act in activation_function:\n",
    "            act = act.lower()\n",
    "            \n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.layers = []\n",
    "        # self.activation_function = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = loss\n",
    "        \n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        # self.y_pred = None\n",
    "\n",
    "        # initialize the input layer\n",
    "        self.layers.append(InputLayer(input_size))\n",
    "        \n",
    "        hidden_layer_sizes.insert(0, input_size)\n",
    "        # +1 for the ones we are inserting in X\n",
    "        \n",
    "        # ic(hidden_layer_sizes)\n",
    "        \n",
    "        for i in range(1, len(hidden_layer_sizes)):\n",
    "            # ic(i)\n",
    "            self.layers.append(Layer(no_of_inputs=hidden_layer_sizes[i-1],no_of_neurons= hidden_layer_sizes[i], activation=self.act_dict[activation_function[i - 1]]))\n",
    "\n",
    "        self.layers.append(OutputLayer(hidden_layer_sizes[-1], output_size, self.act_dict[output_activation_function]))\n",
    "        # print(len(self.layers))\n",
    "    \n",
    "    def forward_propagation(self, x)-> None:\n",
    "        # Forward propagation\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "        # return Softmax().activation(x)\n",
    "        # return x\n",
    "    \n",
    "    # def backward_propagation(self, x, y)-> None:\n",
    "    \n",
    "    def backward_propagation(self, y_pred, y_true)-> None:\n",
    "        # Forward propagation\n",
    "        # self.forward_propagation(x)\n",
    "        \n",
    "        # calculate the loss gradient with respect to the output of the last layer\n",
    "        # which is basically dj_dz for the output layer\n",
    "        # delta_L represents delta for the last layer\n",
    "        \n",
    "        delta_L = self.loss.gradient(y_pred, y_true)\n",
    "        # print(delta_L)\n",
    "        # grad is dj_dz for the output layer\n",
    "        # Backpropagation\n",
    "        delta = delta_L\n",
    "        # for layer in reversed(self.layers):\n",
    "        #     delta = layer.backward(delta)\n",
    "        # do the backword propagation for output layer explicitly and then the hidden layers and then for the input layer\n",
    "        delta = self.layers[-1].backward(delta)\n",
    "        # do the backword propagation for hidden layers\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = self.layers[i].backward(delta, self.layers[i+1].weights)\n",
    "        # do the backword propagation for input layer\n",
    "        delta = self.layers[0].backward(delta)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # Forward propagation\n",
    "        return self.forward_propagation(x)\n",
    "    \n",
    "    def update_weights(self):\n",
    "        for layer in self.layers:\n",
    "            layer.update_weights(self.learning_rate)\n",
    "        \n",
    "    \n",
    "    # Working version of train\n",
    "    # def train(self, X_train, y_train, X_val, y_val, epochs, batch_size):\n",
    "    #     self.losses = []\n",
    "    #     for epoch in range(epochs):\n",
    "    #         loss = []\n",
    "    #         for i in range(0, X_train.shape[0], batch_size):\n",
    "    #             X_batch = X_train[i:i+batch_size]\n",
    "    #             y_batch = y_train[i:i+batch_size]\n",
    "    #             y_pred = self.forward_propagation(X_batch)\n",
    "    #             self.backward_propagation(y_pred=y_pred, y_true=y_batch)\n",
    "    #             self.update_weights()\n",
    "    #             # calculate the loss\n",
    "    #             loss.append(self.loss.loss(y_pred, y_batch))\n",
    "    #         self.losses.append(np.mean(loss))\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs, batch_size=None):\n",
    "        if self.optimizer == \"bgd\":\n",
    "            self.train_bgd(X_train, y_train, X_val, y_val, epochs)\n",
    "        if self.optimizer == \"sgd\":\n",
    "            self.train_sgd(X_train, y_train, X_val, y_val, epochs)\n",
    "        if self.optimizer == \"mbgd\":            \n",
    "            self.train_mbgd(X_train, y_train, X_val, y_val, epochs, batch_size)\n",
    "            \n",
    "    def train_sgd(self, X_train, y_train, X_val, y_val, epochs):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        for epoch in range(epochs):\n",
    "            # generate a random index\n",
    "            for i in range(X_train.shape[0]):\n",
    "                k = np.random.randint(0, X_train.shape[0])\n",
    "                X_batch = X_train[k:k+1]\n",
    "                y_batch = y_train[k:k+1]\n",
    "                y_pred = self.forward_propagation(X_batch)\n",
    "                self.backward_propagation(y_pred=y_pred, y_true=y_batch)\n",
    "                self.update_weights()\n",
    "            # calculate the loss for the entire training set\n",
    "            y_pred_train = self.forward_propagation(X_train)\n",
    "            loss_train = self.loss.loss(y_pred_train, y_train)\n",
    "            self.losses.append(loss_train)\n",
    "            \n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_pred_val = self.forward_propagation(X_val)\n",
    "                loss_val = self.loss.loss(y_pred_val, y_val)\n",
    "                self.val_losses.append(loss_val)\n",
    "        \n",
    "            \n",
    "    def train_bgd(self, X_train, y_train, X_val, y_val, epochs):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward_propagation(X_train)\n",
    "            self.backward_propagation(y_pred=y_pred, y_true=y_train)\n",
    "            self.update_weights()\n",
    "            # calculate the loss for the entire training set\n",
    "            y_pred_train = self.forward_propagation(X_train)\n",
    "            loss_train = self.loss.loss(y_pred_train, y_train)\n",
    "            self.losses.append(loss_train)\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_pred_val = self.forward_propagation(X_val)\n",
    "                loss_val = self.loss.loss(y_pred_val, y_val)\n",
    "                self.val_losses.append(loss_val)\n",
    "            \n",
    "    \n",
    "    def train_mbgd(self, X_train, y_train, X_val, y_val, epochs, batch_size):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, X_train.shape[0], batch_size):\n",
    "                X_batch = X_train[i:i+batch_size]\n",
    "                y_batch = y_train[i:i+batch_size]\n",
    "                y_pred = self.forward_propagation(X_batch)\n",
    "                self.backward_propagation(y_pred=y_pred, y_true=y_batch)\n",
    "                self.update_weights()\n",
    "            # calculate the loss for the entire training set\n",
    "            y_pred_train = self.forward_propagation(X_train)\n",
    "            loss_train = self.loss.loss(y_pred_train, y_train)\n",
    "            self.losses.append(loss_train)\n",
    "            \n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_pred_val = self.forward_propagation(X_val)\n",
    "                loss_val = self.loss.loss(y_pred_val, y_val)\n",
    "                self.val_losses.append(loss_val)           \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = './dataset/WineQT.csv'\n",
    "df = pd.read_csv(PATH)\n",
    "# drop the Id column\n",
    "df = df.drop('Id', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the data except the target column 'quality'\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df.drop('quality', axis=1))\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=df.columns[:-1])\n",
    "\n",
    "quality_min = df['quality'].min()\n",
    "# print(quality_min)\n",
    "df_scaled['quality'] = df['quality'].apply(lambda x: x - quality_min)\n",
    "# df_scaled.head()\n",
    "# df_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1143, 11)\n",
      "(1143, 6)\n"
     ]
    }
   ],
   "source": [
    "df_scaled = pd.get_dummies(df_scaled, columns=['quality'])\n",
    "\n",
    "X = df_scaled.drop(['quality_0', 'quality_1', 'quality_2', 'quality_3', \\\n",
    "                    'quality_4', 'quality_5'], axis=1)\\\n",
    "                        .values\n",
    "\n",
    "y = df_scaled[['quality_0', 'quality_1', 'quality_2', 'quality_3',\\\n",
    "    'quality_4', 'quality_5']]\\\n",
    "        .values\n",
    "        \n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split x into train, test and val set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Classification\n",
    "output_size > 1\n",
    "\n",
    "loss function -> cross entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_size = X.shape[1]\n",
    "output_size = y.shape[1]\n",
    "\n",
    "# possible values for activation functions = \"tanh\", \"sigmoid\", \"relu\", \"identity\", \"softmax\" \n",
    "#                                                                   for regression , for classification\n",
    "# \n",
    "# possible values for optimizer = \"sgd\", \"bgd\", \"mbgd\"\n",
    "\n",
    "\n",
    "# batch_size is only needed when optimizer is \"mbgd\"\n",
    "\n",
    "# Mini-Batch Gradient Descent\n",
    "# mbgd\n",
    "# mlp = MLP(input_size=input_size, hidden_layer_sizes=[32, 32], output_size=output_size, \\\n",
    "#                     activation_function=[\"relu\", \"relu\"], output_activation_function=\"softmax\",\\\n",
    "#                         optimizer=\"mbgd\", loss=CrossEntropyLoss(), learning_rate=0.01)\n",
    "# mlp.train(X_train, y_train, X_val, y_val, epochs=10000, batch_size=32)\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "# sgd\n",
    "# mlp = MLP(input_size=input_size, hidden_layer_sizes=[32, 32], output_size=output_size, \\\n",
    "#                     activation_function=[\"relu\", \"relu\"], output_activation_function=\"softmax\",\\\n",
    "#                         optimizer=\"sgd\", loss=CrossEntropyLoss(), learning_rate=0.01)\n",
    "# mlp.train(X_train, y_train, X_val, y_val, epochs=1000)\n",
    "\n",
    "# Batch Gradient Descent\n",
    "# bgd\n",
    "mlp = MLP(input_size=input_size, hidden_layer_sizes=[32, 32], output_size=output_size, \\\n",
    "                    activation_function=[\"relu\", \"relu\"], output_activation_function=\"softmax\",\\\n",
    "                        optimizer=\"bgd\", loss=CrossEntropyLoss(), learning_rate=0.01)\n",
    "mlp.train(X_train, y_train, X_val, y_val, epochs=2000)\n",
    "\n",
    "plt.plot(range(1, len(mlp.losses) + 1), mlp.losses, color='b', label='Training Loss')\n",
    "plt.plot(range(1, len(mlp.val_losses) + 1), mlp.val_losses, color='r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Model Training & Hyperparameter Tuning using W&B\n",
    "# [10 marks]\n",
    "# Effective tuning can vastly improve a model’s performance. Integrate Weights\n",
    "# & Biases (W&B) to log and track your model’s metrics. Using W&B and your\n",
    "# validation set, experiment with hyperparameters such as learning rate, epochs,\n",
    "# hidden layer neurons, activation functions, and optimization techniques. You\n",
    "# have to use W&B for loss tracking during training and to log effects of different\n",
    "# activation functions and optimizers on model performance.\n",
    "# 1. Log your scores - loss and accuracy on validation set and train set using\n",
    "# W&B.\n",
    "# 2. Report metrics: accuracy, f-1 score, precision, and recall. You are allowed\n",
    "# to use sklearn metrics for this part.\n",
    "# 3. You have to report the scores(ordered) for all the combinations of :\n",
    "# • Activation functions : sigmoid, tanh and ReLU (implemented from\n",
    "# scratch)\n",
    "# • Optimizers : SGD, batch gradient descent, and mini-batch gradient\n",
    "# descent (implemented from scratch).\n",
    "# 4. Tune your model on various hyperparameters, such as learning rate, epochs,\n",
    "# and hidden layer neurons.\n",
    "# • Plot the trend of accuracy scores with change in these hyperparam-\n",
    "# eters.\n",
    "# • Report the parameters for the best model that you get (for the various\n",
    "# values you trained the model on).\n",
    "# • Report the scores mentioned in 2.2.2 for all values of hyperparameters\n",
    "# in a table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Regression\n",
    "\n",
    "loss function -> mean squared error\n",
    "\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = './dataset/WineQT.csv'\n",
    "# df = pd.read_csv(PATH)\n",
    "# # drop the Id column\n",
    "# df = df.drop('Id', axis=1)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# df_scaled = scaler.fit_transform(df.drop('quality', axis=1))\n",
    "# df_scaled = pd.DataFrame(df_scaled, columns=df.columns[:-1])\n",
    "\n",
    "# quality_min = df['quality'].min()\n",
    "# # print(quality_min)\n",
    "# df_scaled['quality'] = df['quality'].apply(lambda x: x - quality_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df_scaled.drop(['quality'], axis=1)\\\n",
    "#                         .values\n",
    "\n",
    "# y = df_scaled[['quality']]\\\n",
    "#         .values\n",
    "        \n",
    "# print(X.shape)\n",
    "# print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
