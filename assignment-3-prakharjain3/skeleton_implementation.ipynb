{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, x):\n",
    "        pass\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def backward(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "class Tanh(ActivationFunction):\n",
    "    def forward(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def backward(self, x):\n",
    "        return 1 - x**2\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    def forward(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, x):\n",
    "        return (x > 0).astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update(self, layer):\n",
    "        pass\n",
    "    \n",
    "class BatchGradientDescent(Optimizer):\n",
    "    def update(self, layer, gradient_weights, gradient_biases):\n",
    "        layer.weights -= self.learning_rate * gradient_weights\n",
    "        layer.biases -= self.learning_rate * gradient_biases\n",
    "\n",
    "\n",
    "class StochasticGradientDescent(Optimizer):\n",
    "    def update(self, layer, gradient_weights, gradient_biases):\n",
    "        layer.weights -= self.learning_rate * gradient_weights\n",
    "        layer.biases -= self.learning_rate * gradient_biases\n",
    "\n",
    "class MiniBatchGradientDescent(Optimizer):\n",
    "    def __init__(self, learning_rate, batch_size):\n",
    "        super().__init__(learning_rate)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def update(self, layer, gradient_weights, gradient_biases):\n",
    "        layer.weights -= self.learning_rate * gradient_weights / self.batch_size\n",
    "        layer.biases -= self.learning_rate * gradient_biases / self.batch_size\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        super().__init__(learning_rate)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = 0\n",
    "        self.v = 0\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, layer):\n",
    "        self.t += 1\n",
    "        layer.gradient_weights = layer.gradient_weights + self.epsilon\n",
    "        layer.gradient_biases = layer.gradient_biases + self.epsilon\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * layer.gradient_weights\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (layer.gradient_weights ** 2)\n",
    "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
    "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
    "        layer.weights -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        layer.biases -= self.learning_rate * layer.gradient_biases / (np.sqrt(v_hat) + self.epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, input_size):\n",
    "        self.weights = np.random.rand(input_size)\n",
    "        self.biases = np.random.rand(1)\n",
    "        self.activation = None\n",
    "        self.gradient_weights = None\n",
    "        self.gradient_biases = None\n",
    "    \n",
    "    def set_activation(self, activation):\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.weights) + self.biases\n",
    "        return self.activation.forward(z)\n",
    "    \n",
    "    def backward(self, x, delta):\n",
    "        self.gradient_weights = np.dot(x.T, delta)\n",
    "        self.gradient_biases = np.sum(delta, axis=0)\n",
    "        return np.dot(delta, self.weights.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.perceptrons = [Perceptron(input_size) for _ in range(output_size)]\n",
    "    \n",
    "    def set_activation(self, activation):\n",
    "        for perceptron in self.perceptrons:\n",
    "            perceptron.set_activation(activation)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return np.array([perceptron.forward(x) for perceptron in self.perceptrons])\n",
    "    \n",
    "    def backward(self, x, delta):\n",
    "        return np.array([perceptron.backward(x, delta) for perceptron in self.perceptrons])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier:\n",
    "    def __init__(self, input_size, hidden_layer_sizes, output_size, learning_rate, activation, optimizer):\n",
    "        self.layers = []\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Create input layer\n",
    "        input_layer = PerceptronLayer(input_size, hidden_layer_sizes[0])\n",
    "        input_layer.set_activation(activation)\n",
    "        self.layers.append(input_layer)\n",
    "        \n",
    "        # Create hidden layers\n",
    "        for i in range(1, len(hidden_layer_sizes)):\n",
    "            hidden_layer = PerceptronLayer(hidden_layer_sizes[i - 1], hidden_layer_sizes[i])\n",
    "            hidden_layer.set_activation(activation)\n",
    "            self.layers.append(hidden_layer)\n",
    "        \n",
    "        # Create output layer\n",
    "        output_layer = PerceptronLayer(hidden_layer_sizes[-1], output_size)\n",
    "        output_layer.set_activation(activation)\n",
    "        self.layers.append(output_layer)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, x, y):\n",
    "        # Backpropagation\n",
    "        error = y - x\n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            error = self.layers[i].backward(x, error)\n",
    "    \n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                target = y[i]\n",
    "                prediction = self.forward(x)\n",
    "                self.backward(prediction, target)\n",
    "                for layer in self.layers:\n",
    "                    self.optimizer.update(layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given By ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation Functions\n",
    "class ActivationFunction:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, x):\n",
    "        pass\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def backward(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "class Tanh(ActivationFunction):\n",
    "    def forward(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def backward(self, x):\n",
    "        return 1 - x**2\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    def forward(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "# Optimizers\n",
    "class Optimizer:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update(self, layer):\n",
    "        pass\n",
    "\n",
    "class BatchGradientDescent(Optimizer):\n",
    "    def update(self, layer):\n",
    "        layer.weights -= self.learning_rate * layer.gradient_weights\n",
    "        layer.biases -= self.learning_rate * layer.gradient_biases\n",
    "\n",
    "class StochasticGradientDescent(Optimizer):\n",
    "    def update(self, layer):\n",
    "        layer.weights -= self.learning_rate * layer.gradient_weights\n",
    "        layer.biases -= self.learning_rate * layer.gradient_biases\n",
    "\n",
    "class MiniBatchGradientDescent(Optimizer):\n",
    "    def __init__(self, learning_rate, batch_size):\n",
    "        super().__init__(learning_rate)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def update(self, layer):\n",
    "        layer.weights -= self.learning_rate * layer.gradient_weights / self.batch_size\n",
    "        layer.biases -= self.learning_rate * layer.gradient_biases / self.batch_size\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        super().__init__(learning_rate)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = 0\n",
    "        self.v = 0\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, layer):\n",
    "        self.t += 1\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * layer.gradient_weights\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (layer.gradient_weights ** 2)\n",
    "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
    "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
    "        layer.weights -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        layer.biases -= self.learning_rate * layer.gradient_biases / (np.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "# Perceptron\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size):\n",
    "        self.weights = np.random.rand(input_size)\n",
    "        self.biases = np.random.rand(1)\n",
    "        self.activation = None\n",
    "        self.gradient_weights = None\n",
    "        self.gradient_biases = None\n",
    "    \n",
    "    def set_activation(self, activation):\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.weights) + self.biases\n",
    "        return self.activation.forward(z)\n",
    "    \n",
    "    def backward(self, x, delta):\n",
    "        self.gradient_weights = np.dot(x.T, delta)\n",
    "        self.gradient_biases = np.sum(delta, axis=0)\n",
    "        return np.dot(delta, self.weights.T)\n",
    "\n",
    "# Perceptron Layer\n",
    "class PerceptronLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.perceptrons = [Perceptron(input_size) for _ in range(output_size)]\n",
    "    \n",
    "    def set_activation(self, activation):\n",
    "        for perceptron in self.perceptrons:\n",
    "            perceptron.set_activation(activation)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return np.array([perceptron.forward(x) for perceptron in self.perceptrons])\n",
    "    \n",
    "    def backward(self, x, delta):\n",
    "        return np.array([perceptron.backward(x, delta) for perceptron in self.perceptrons])\n",
    "\n",
    "# Multi Layer Perceptron\n",
    "class MLPClassifier:\n",
    "    def __init__(self, input_size, hidden_layer_sizes, output_size, learning_rate, activation, optimizer):\n",
    "        self.layers = []\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Create input layer\n",
    "        input_layer = PerceptronLayer(input_size, hidden_layer_sizes[0])\n",
    "        input_layer.set_activation(activation)\n",
    "        self.layers.append(input_layer)\n",
    "        \n",
    "        # Create hidden layers\n",
    "        for i in range(1, len(hidden_layer_sizes)):\n",
    "            hidden_layer = PerceptronLayer(hidden_layer_sizes[i - 1], hidden_layer_sizes[i])\n",
    "            hidden_layer.set_activation(activation)\n",
    "            self.layers.append(hidden_layer)\n",
    "        \n",
    "        # Create output layer\n",
    "        output_layer = PerceptronLayer(hidden_layer_sizes[-1], output_size)\n",
    "        output_layer.set_activation(activation)\n",
    "        self.layers.append(output_layer)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, x, y):\n",
    "        # Backpropagation\n",
    "        error = y - x\n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            error = self.layers[i].backward(x, error)\n",
    "    \n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                target = y[i]\n",
    "                prediction = self.forward(x)\n",
    "                self.backward(prediction, target)\n",
    "                for layer in self.layers:\n",
    "                    self.optimizer.update(layer)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
