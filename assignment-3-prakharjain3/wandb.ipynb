{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/prakhar/Desktop/assignment-3-prakharjain3/wandb/run-20230930_035205-1kj5ajcz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/prakhar-jain/my-awesome-project/runs/1kj5ajcz' target=\"_blank\">solar-wood-2</a></strong> to <a href='https://wandb.ai/prakhar-jain/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/prakhar-jain/my-awesome-project' target=\"_blank\">https://wandb.ai/prakhar-jain/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/prakhar-jain/my-awesome-project/runs/1kj5ajcz' target=\"_blank\">https://wandb.ai/prakhar-jain/my-awesome-project/runs/1kj5ajcz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▄▆▇▇███</td></tr><tr><td>loss</td><td>█▅▂▂▁▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.91858</td></tr><tr><td>loss</td><td>0.09035</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">solar-wood-2</strong> at: <a href='https://wandb.ai/prakhar-jain/my-awesome-project/runs/1kj5ajcz' target=\"_blank\">https://wandb.ai/prakhar-jain/my-awesome-project/runs/1kj5ajcz</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230930_035205-1kj5ajcz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "# simulate training\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "    \n",
    "    # log metrics to wandb\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "    \n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X = [[X11, X12, X13, X14, X15, ....... X1n],\n",
    "     [X21, X22, X23, X24, X25, ....... X2n],\n",
    "     [X31, X32, X33, X34, X35, ....... X3n],\n",
    "     ......................................,\n",
    "     [Xm1, Xm2, Xm3, Xm4, Xm5, ....... Xmn]]\n",
    "\n",
    "W = [W1, W2, W3, W4, W5, ....... Wn] # weights for each feature\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'wandb' has no attribute 'logout'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/prakhar/Desktop/assignment-3-prakharjain3/wandb.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/prakhar/Desktop/assignment-3-prakharjain3/wandb.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m wandb\u001b[39m.\u001b[39;49mlogout()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'wandb' has no attribute 'logout'"
     ]
    }
   ],
   "source": [
    "# class MultiomialLogisticRegression:\n",
    "#     def __init__(self, lr=0.01, epochs=2000):\n",
    "#         self.lr = lr\n",
    "#         self.epochs = epochs\n",
    "        \n",
    "#     def init_params(self, X, y):\n",
    "#         self.weights = np.random.randn(y.shape[1], X.shape[1]) # (no of classes, no of features)\n",
    "#         self.k = y.shape[1]\n",
    "#         self.n = X.shape[0]\n",
    "#         self.m = X.shape[1]\n",
    "        \n",
    "#     def fit(self, X, y, X_val=None, y_val=None):\n",
    "#         X = np.insert(X, 0, 1, axis=1)\n",
    "#         self.init_params(X, y)\n",
    "        \n",
    "#         self.loss = []\n",
    "#         for i in range(self.epochs):\n",
    "#             soft = self.softmax(self.calculate_z(X))\n",
    "#             grad = self.gradient(X, y, soft)\n",
    "#             self.weights -= self.lr * grad\n",
    "            \n",
    "#             self.loss.append(self.cross_entropy_loss(y, soft))\n",
    "#             # print(f\"Epoch {i+1}: loss = {self.loss[-1]}\")\n",
    "                \n",
    "#         return self.weights, self.loss\n",
    "    \n",
    "#     def softmax(self, z):\n",
    "#         z -= np.max(z, axis=1, keepdims=True) # for numerical stability\n",
    "#         return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "    \n",
    "#     def calculate_z(self, X):\n",
    "#         return np.dot(X, self.weights.T)\n",
    "    \n",
    "#     def cross_entropy_loss(self, y, y_hat):\n",
    "#         loss = -np.sum(y * np.log(y_hat)) / y.shape[0]\n",
    "#         return loss\n",
    "    \n",
    "#     def gradient(self, X, y, y_hat):\n",
    "#         grad = np.dot((y_hat - y).T, X) / self.n\n",
    "#         return grad\n",
    "\n",
    "\n",
    "# MLR = MultiomialLogisticRegression()\n",
    "# MLR.fit(X_train, y_train)\n",
    "# plt.plot(MLR.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skeleton Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Skeleton Implementation\n",
    "# # Import necessary libraries\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import classification_report\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# class MultinomialLogisticRegression:\n",
    "#     def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True):\n",
    "#         self.lr = lr\n",
    "#         self.num_iter = num_iter\n",
    "#         self.fit_intercept = fit_intercept\n",
    "    \n",
    "#     def add_intercept(self, X):\n",
    "#         intercept = np.ones((X.shape[0], 1))\n",
    "#         return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "#     def softmax(self, z):\n",
    "#         return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "    \n",
    "#     def cross_entropy_loss(self, y, y_hat):\n",
    "#         return -np.mean(np.sum(y * np.log(y_hat), axis=1))\n",
    "    \n",
    "#     def gradient_descent(self, X, y, y_hat):\n",
    "#         gradient = np.dot(X.T, (y_hat - y)) / y.shape[0]\n",
    "#         self.theta -= self.lr * gradient\n",
    "    \n",
    "#     def fit(self, X, y):\n",
    "#         if self.fit_intercept:\n",
    "#             X = self.add_intercept(X)\n",
    "        \n",
    "#         self.theta = np.zeros((X.shape[1], y.shape[1]))\n",
    "        \n",
    "#         for i in range(self.num_iter):\n",
    "#             z = np.dot(X, self.theta)\n",
    "#             y_hat = self.softmax(z)\n",
    "#             loss = self.cross_entropy_loss(y, y_hat)\n",
    "            \n",
    "#             if i % 10000 == 0:\n",
    "#                 print(f'Iteration {i}, Loss: {loss}')\n",
    "#                 y_pred = np.argmax(y_hat, axis=1)\n",
    "#                 print(classification_report(np.argmax(y, axis=1), y_pred))\n",
    "            \n",
    "#             self.gradient_descent(X, y, y_hat)\n",
    "    \n",
    "#     def predict(self, X):\n",
    "#         if self.fit_intercept:\n",
    "#             X = self.add_intercept(X)\n",
    "        \n",
    "#         return np.argmax(self.softmax(np.dot(X, self.theta)), axis=1)\n",
    "\n",
    "# # Train the model\n",
    "# X_train = # training data\n",
    "# y_train = # training labels\n",
    "# X_val = # validation data\n",
    "# y_val = # validation labels\n",
    "\n",
    "# model = MultinomialLogisticRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Print loss and accuracy on train set\n",
    "# y_train_pred = model.predict(X_train)\n",
    "# train_loss = model.cross_entropy_loss(y_train, model.softmax(np.dot(model.add_intercept(X_train), model.theta)))\n",
    "# train_acc = np.mean(y_train_pred == np.argmax(y_train, axis=1))\n",
    "# print(f'Train Loss: {train_loss}, Train Accuracy: {train_acc}')\n",
    "\n",
    "# # Plot decision boundaries\n",
    "# feature1 = # feature 1 data\n",
    "# feature2 = # feature 2 data\n",
    "# x_min, x_max = feature1.min() - 1, feature1.max() + 1\n",
    "# y_min, y_max = feature2.min() - 1, feature2.max() + 1\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "# Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "# Z = Z.reshape(xx.shape)\n",
    "# plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "# plt.scatter(feature1, feature2, c=np.argmax(y_train, axis=1), alpha=0.8)\n",
    "# plt.xlabel('Feature 1')\n",
    "# plt.ylabel('Feature 2')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# class MultinomialLogisticRegression:\n",
    "#     def __init__(self, learning_rate=0.01, epochs=1000):\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.epochs = epochs\n",
    "    \n",
    "#     def _softmax(self, z):\n",
    "#         exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "#         return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "#     def _cross_entropy_loss(self, y_true, y_pred):\n",
    "#         m = y_true.shape[0]\n",
    "#         loss = -np.sum(y_true * np.log(y_pred)) / m\n",
    "#         return loss\n",
    "    \n",
    "#     def _gradient_descent(self, X, y, y_pred):\n",
    "#         m = y.shape[0]\n",
    "#         dw = (1 / m) * np.dot(X.T, (y_pred - y))\n",
    "#         db = (1 / m) * np.sum(y_pred - y, axis=0)\n",
    "#         return dw, db\n",
    "    \n",
    "#     def fit(self, X, y, X_val=None, y_val=None):\n",
    "#         n_features = X.shape[1]\n",
    "#         n_classes = y.shape[1]\n",
    "#         self.weights = np.zeros((n_features, n_classes))\n",
    "#         self.bias = np.zeros(n_classes)\n",
    "#         self.losses = []\n",
    "#         self.val_losses = []\n",
    "#         self.accuracies = []\n",
    "#         self.val_accuracies = []\n",
    "        \n",
    "#         for epoch in range(self.epochs):\n",
    "#             z = np.dot(X, self.weights) + self.bias\n",
    "#             y_pred = self._softmax(z)\n",
    "#             loss = self._cross_entropy_loss(y, y_pred)\n",
    "#             self.losses.append(loss)\n",
    "#             accuracy = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1))\n",
    "#             self.accuracies.append(accuracy)\n",
    "            \n",
    "#             if X_val is not None and y_val is not None:\n",
    "#                 z_val = np.dot(X_val, self.weights) + self.bias\n",
    "#                 y_pred_val = self._softmax(z_val)\n",
    "#                 val_loss = self._cross_entropy_loss(y_val, y_pred_val)\n",
    "#                 self.val_losses.append(val_loss)\n",
    "#                 val_accuracy = np.mean(np.argmax(y_pred_val, axis=1) == np.argmax(y_val, axis=1))\n",
    "#                 self.val_accuracies.append(val_accuracy)\n",
    "                \n",
    "#             dw, db = self._gradient_descent(X, y, y_pred)\n",
    "#             self.weights -= self.learning_rate * dw\n",
    "#             self.bias -= self.learning_rate * db\n",
    "    \n",
    "#     def predict(self, X):\n",
    "#         z = np.dot(X, self.weights) + self.bias\n",
    "#         y_pred = self._softmax(z)\n",
    "#         return y_pred\n",
    "    \n",
    "#     def plot_decision_boundary(self, X, y, feature1, feature2):\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         x_min, x_max = X[:, feature1].min() - 1, X[:, feature1].max() + 1\n",
    "#         y_min, y_max = X[:, feature2].min() - 1, X[:, feature2].max() + 1\n",
    "#         xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "#         Z = np.argmax(self.predict(np.c_[xx.ravel(), yy.ravel()]), axis=1)\n",
    "#         Z = Z.reshape(xx.shape)\n",
    "#         plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "#         plt.scatter(X[:, feature1], X[:, feature2], c=np.argmax(y, axis=1), alpha=0.8)\n",
    "#         plt.xlabel('Feature {}'.format(feature1))\n",
    "#         plt.ylabel('Feature {}'.format(feature2))\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # Generate a random dataset\n",
    "# X, y = make_classification(n_samples=1000, n_features=5, n_classes=3, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# # Split the dataset into train, validation, and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# # One-hot encode the target variable\n",
    "# enc = OneHotEncoder()\n",
    "# y_train = enc.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "# y_val = enc.transform(y_val.reshape(-1, 1)).toarray()\n",
    "# y_test = enc.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "# # Create an instance of the MultinomialLogisticRegression class\n",
    "# model = MultinomialLogisticRegression(learning_rate=0.1, epochs=1000)\n",
    "\n",
    "# # Train the model on the train set and validate on the validation set\n",
    "# model.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# # Print the loss and accuracy on the train set\n",
    "# train_loss = model.losses[-1]\n",
    "# train_accuracy = model.accuracies[-1]\n",
    "# print('Train Loss:', train_loss)\n",
    "# print('Train Accuracy:', train_accuracy)\n",
    "\n",
    "# # Print the classification report on the test set\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred = np.argmax(y_pred, axis=1)\n",
    "# y_test = np.argmax(y_test, axis=1)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# # Plot the decision boundary using the two significant features identified in Task 1\n",
    "# model.plot_decision_boundary(X_train, y_train, feature1=0, feature2=1)\n",
    "\n",
    "# # Fine-tune the hyperparameters using the validation set and W&B logging\n",
    "# # Evaluate the model on the test set and print the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Simple Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiomialLogisticRegression:\n",
    "#     def __init__(self, lr=0.01, epochs=2000):\n",
    "#         self.lr = lr\n",
    "#         self.epochs = epochs\n",
    "        \n",
    "#     def init_params(self, X, y):\n",
    "#         self.weights = np.random.randn(y.shape[1], X.shape[1]) # (no of classes, no of features)\n",
    "#         self.k = y.shape[1]\n",
    "#         self.n = X.shape[0]\n",
    "#         self.m = X.shape[1]\n",
    "        \n",
    "#     def fit(self, X, y, X_val=None, y_val=None):\n",
    "#         X = np.insert(X, 0, 1, axis=1) # insert 1 for bias\n",
    "        \n",
    "#         self.init_params(X, y)\n",
    "        \n",
    "#         self.loss = []\n",
    "#         self.accuracy = []\n",
    "#         for i in range(self.epochs):\n",
    "#             # Calculate the gradient and update the weights\n",
    "#             indice = np.random.randint(0, self.n)\n",
    "#             soft = self.softmax(self.calculate_z(X[indice:indice+1]))\n",
    "#             grad = self.gradient(X[indice:indice+1], y[indice], soft)\n",
    "#             self.weights -= self.lr * grad\n",
    "            \n",
    "#             # Calculate the loss and store it\n",
    "#             self.loss.append(self.cross_entropy_loss(y, soft))\n",
    "            \n",
    "#             if X_val is not None and y_val is not None:\n",
    "#                 y_pred = self.predict(X_val)\n",
    "#                 self.accuracy.append(accuracy_score(y_true = np.argmax(y_val , axis=1), y_pred= y_pred))\n",
    "                \n",
    "#         # return self.weights, self.loss\n",
    "    \n",
    "#     def softmax(self, z):\n",
    "#         z -= np.max(z, axis=1, keepdims=True) # for numerical stability\n",
    "#         return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "    \n",
    "#     def calculate_z(self, X):\n",
    "#         return np.dot(X, self.weights.T)\n",
    "    \n",
    "#     def cross_entropy_loss(self, y, y_hat):\n",
    "#         loss = -np.sum(y * np.log(y_hat)) / y.shape[0]\n",
    "#         return loss\n",
    "    \n",
    "#     def gradient(self, X, y, y_hat):\n",
    "#         grad = np.dot((y_hat - y).T, X) / X.shape[0]\n",
    "#         return grad\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         X = np.insert(X, 0, 1, axis=1)\n",
    "#         z = self.calculate_z(X)\n",
    "#         y_pred = np.argmax(self.softmax(z), axis=1)\n",
    "#         return y_pred\n",
    "    \n",
    "# MLR = MultiomialLogisticRegression(epochs=2000)\n",
    "# MLR.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            y_pred = self.forward_propagation(X_train)\n",
    "            # calculate the loss for each epoch\n",
    "            loss = self.loss.loss(y_pred, y_train)\n",
    "            # print the loss for each epoch\n",
    "            self.losses.append(loss)\n",
    "            print(f\"Epoch {epoch} loss: {loss}\")\n",
    "            y_val_pred = self.forward_propagation(X_val)\n",
    "            y_val_pred = np.argmax(y_val_pred, axis=1)\n",
    "            \n",
    "            # print(y_val_pred)\n",
    "            \n",
    "            print(f\"Validation Accuracy: {accuracy_score(np.argmax(y_val, axis=1), y_val_pred)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
